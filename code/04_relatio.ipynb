{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee00a3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `relatio` for analysis of narratives in the survey data\n",
    "**Runtime $\\sim$ 10min**\n",
    "\n",
    "----------------------------\n",
    "\n",
    "This code is based on a short demo of the package `relatio` publicly available from the authors of the relatio method. I use the the main wrapper functions to quickly obtain narrative statements from the corpus of my survey responses. For further details, please refer to the paper: [\"Text Semantics Capture Political and Economic Narratives\"](https://arxiv.org/abs/2108.01720) \n",
    "\n",
    "Notes on inputs and outputs:\n",
    "It takes as input a text corpus and outputs a list of narrative statements. The pipeline is unsupervised: the user does not need to specify narratives beforehand. Narrative statements are defined as tuples of semantic roles with a (agent, verb, patient, attribute) structure. \n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421da5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "\n",
    "survey_data_filtered_file = '/Users/emilyrobitschek/git/ETH/SPUR/densification_project/datasets/spur_survey_response_filtered_df.txt'\n",
    "survey_df = pd.read_csv(survey_data_filtered_file, sep='\\t')\n",
    "\n",
    "subset_col = 'Q12.6_corrected'\n",
    "df = survey_df[['ResponseId', subset_col]]\n",
    "df = df.rename(columns={'ResponseId': 'id', subset_col : 'doc'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stop words from ntlk (less extensive list):\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stops = stopwords.words('english')\n",
    "nltk_stops.extend(['u'])\n",
    "print(nltk_stops)\n",
    "nltk_stops = set(nltk_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac363a9",
   "metadata": {},
   "source": [
    "## Step 1: Split into sentences\n",
    "\n",
    "----------------------------\n",
    "\n",
    "For any new corpus, the first thing you will want to do is to split the corpus into sentences.\n",
    "\n",
    "We do this on the first 100 tweets. \n",
    "\n",
    "The output is two lists: one with an index for the document and one with the resulting split sentences.\n",
    "\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41183920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.utils import split_into_sentences\n",
    "\n",
    "split_sentences = split_into_sentences(\n",
    "    df, progress_bar=True\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    print('Document id: %s' %split_sentences[0][i])\n",
    "    print('Sentence: %s \\n' %split_sentences[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b8592",
   "metadata": {},
   "source": [
    "## Step 2: Annotate semantic roles\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Once the corpus is split into sentences. You can feed it to the semantic role labeler.\n",
    "\n",
    "The output is a list of json objects which contain the semantic role annotations for each sentence in the corpus.\n",
    "\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72d3df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that SRL is time-consuming, in particular on CPUs.\n",
    "# To speed up the annotation, you can also use GPUs via the \"cuda_device\" argument of the \"run_srl()\" function. \n",
    "\n",
    "from relatio.wrappers import run_srl\n",
    "\n",
    "srl_res = run_srl(\n",
    "    path=\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\", # pre-trained model\n",
    "    sentences=split_sentences[1],\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ffc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of SRL output\n",
    "srl_res[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138321cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e3e8f9f",
   "metadata": {},
   "source": [
    "## Step 3: Build the narrative model\n",
    "\n",
    "----------------------------\n",
    "\n",
    "We are now ready to build a narrative model.\n",
    "\n",
    "The function `build_narrative_model` takes as input the split sentences the SRL annotations for the corpus. It builds a model of low-dimensional narrative statements which may then be used to obtain narrative statements  \"out-of-sample\".\n",
    "\n",
    "The function has sensible defaults for most arguments, but the user should at least specify:\n",
    "- the number of latent unnamed entities to recover (`n_clusters`)\n",
    "- the embeddings to be used (see here for further details) \n",
    "\n",
    "We specify 100 unnamed entities to uncover. The embeddings used are pre-trained glove embeddings.\n",
    "\n",
    "To speed up the model's training, we also focus on the top 100 most frequent named entities (the default is to mine all named entities).\n",
    "\n",
    "To improve interpretability, we remove common uninformative words in the corpus (the \"stopwords\"), as well as one-letter words.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: This step usually takes several minutes to run. You might want to grab a coffee.\n",
    " \n",
    "from relatio.wrappers import build_narrative_model\n",
    "\n",
    "narrative_model = build_narrative_model(\n",
    "    srl_res=srl_res,\n",
    "    sentences=split_sentences[1],\n",
    "    embeddings_type=\"gensim_keyed_vectors\",\n",
    "    embeddings_path=\"glove-wiki-gigaword-300\",\n",
    "    n_clusters=[[50]], \n",
    "    stop_words = nltk_stops,\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The narrative model is simply a dictionary containing the narrative model's specifics.\n",
    "print(narrative_model.keys())\n",
    "print(nltk_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common named entities\n",
    "narrative_model['entities'].most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ea112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unnamed entities uncovered in the corpus \n",
    "# (automatically labeled by the most frequent phrase in the cluster)\n",
    "narrative_model['cluster_labels_most_freq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f9c77",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "In practice, `build_narrative_model` is a flexible wrapper function which gives a lot of control to the user. \n",
    "\n",
    "Let's break the options down into four categories:\n",
    "\n",
    "1. Basic utilities\n",
    "    - the semantic roles you're interested in (`roles_considered`)\n",
    "    - where you would like to save the model (`output_path`)\n",
    "    - whether you cant to track the function's progress (`progress_bar`)\n",
    "    \n",
    "\n",
    "2. Text preprocessing\n",
    "    - basic text preprocessing steps \n",
    "    (`remove_punctuation`, `remove_digits`, remove `stop_words`, `stem` or `lemmatize` words, etc.)\n",
    "    - would you like to replace verbs by their most common synonyms/antonyms? (`dimension_reduce_verbs`)\n",
    "\n",
    "\n",
    "3. Named entities  \n",
    "    - which semantic roles have named entities? (`roles_with_entities`)\n",
    "    - how many named entities would you like to keep? (`top_n_entities`)\n",
    "\n",
    "Technical details: under the hood, we work with SpaCy named entity recognizer to identify named entities. \n",
    "We consider tags related to places, organizations, people and events.\n",
    "\n",
    "\n",
    "4. Unnamed entities (e.g., tax, government, dog, cat, etc.)\n",
    "    - which semantic roles have unnamed entities? (`roles_with_embeddings`)\n",
    "    - how many latent unnamed entities are there in the corpus? (`n_clusters`)\n",
    "\n",
    "Technical details: under the hood, we embed semantic phrases without named entities and cluster them with \n",
    "K-Means. \n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2521a9",
   "metadata": {},
   "source": [
    "## Step 4: Get narrative statements based on the narrative model.\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Once the narrative model is built, we can use it to extract narrative statements from any corpus (provided that the \n",
    "corpus is split into sentences and annotated for semantic roles). \n",
    "\n",
    "We call the function `get_narratives` for this purpose.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6dc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.wrappers import get_narratives\n",
    "\n",
    "final_statements = get_narratives(\n",
    "    srl_res=srl_res,\n",
    "    doc_index=split_sentences[0],  # doc names\n",
    "    narrative_model=narrative_model,\n",
    "    n_clusters=[0],  \n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting pandas dataframe\n",
    "\n",
    "print(final_statements.columns)\n",
    "\n",
    "final_statements.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de4356",
   "metadata": {},
   "source": [
    "### Trying out different clustering scenarios\n",
    "\n",
    "The choice of the number of clusters is corpus and application specific. Specifying a small number of latent entities leads to a large dimension reduction and may decrease entity coherence, whilst specifying a large number of latent entities may produce cluster redundancy. \n",
    "\n",
    "To help users try different clustering scenarios, the wrapper functions `build_narrative_model` and `get_narratives` allow users to experiment various clustering scenarios, which we detail below.\n",
    "\n",
    "----------------------------\n",
    "\n",
    "In `build_narrative_model`, the arguments `roles_with_embeddings` and `n_clusters` are specified as lists of lists arguments. This implies that you can cluster semantic roles separately (or together) and with different numbers of clusters. \n",
    "\n",
    "For example, a user could specify:\n",
    "- `roles_with_embeddings = [[\"ARG0\"],[\"ARG1\"]]`\n",
    "- `n_clusters = [[10,20],[10]]`\n",
    "    \n",
    "He/she would then cluster \"ARG0\" and \"ARG1\" separately and not consider \"ARG2\" for dimension reduction. For \"ARG0\", the clustering scenarios are a model with 10 and a model with 20 clusters. For \"ARG1\", the clustering scenario is a model with 10 clusters.\n",
    "\n",
    "\n",
    "----------------------------\n",
    "\n",
    "To extract narrative statements based on a clustering scenario, the `get_narratives` function also has the \n",
    "`n_clusters` argument. \n",
    "\n",
    "`n_clusters` is a list for the clustering scenarios. For instance, in our previous example, \n",
    "semantic roles \"ARG0\" and \"ARG1\" are clustered separately, so `n_clusters` expects two indices: one for the \n",
    "clustering scenario to pick for \"ARG0\" and one for the clustering scenario to pick for \"ARG1\".\n",
    "\n",
    "For example, a user could specify:\n",
    "- `n_clusters = [0,0]`\n",
    "\n",
    "He/she would then extract narrative statements based on 10 clusters for \"ARG0\" and 10 clusters for \"ARG1\".\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238f719",
   "metadata": {},
   "source": [
    "## Step 5: Model validation and basic analysis\n",
    "\n",
    "----------------------------\n",
    "\n",
    "The resulting `final_statments` object is a pandas dataframe which lists narrative statements found in documents and \n",
    "sentences of the corpus.\n",
    "\n",
    "It is straight-forward to manually inspect the quality of the resulting entities and narrative statements.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e554840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity coherence\n",
    "# Print most frequent phrases per entity\n",
    "\n",
    "# Pool ARG0, ARG1 and ARG2 together\n",
    "\n",
    "df1 = final_statements[['ARG0_lowdim', 'ARG0_highdim']]\n",
    "df1.rename(columns={'ARG0_lowdim': 'ARG', 'ARG0_highdim': 'ARG-RAW'}, inplace=True)\n",
    "\n",
    "df2 = final_statements[['ARG1_lowdim', 'ARG1_highdim']]\n",
    "df2.rename(columns={'ARG1_lowdim': 'ARG', 'ARG1_highdim': 'ARG-RAW'}, inplace=True)\n",
    "\n",
    "df3 = final_statements[['ARG2_lowdim', 'ARG2_highdim']]\n",
    "df3.rename(columns={'ARG2_lowdim': 'ARG', 'ARG2_highdim': 'ARG-RAW'}, inplace=True)\n",
    "\n",
    "df = df1.append(df2).reset_index(drop = True)\n",
    "df = df.append(df3).reset_index(drop = True)\n",
    "\n",
    "# Count semantic phrases\n",
    "\n",
    "df = df.groupby(['ARG', 'ARG-RAW']).size().reset_index()\n",
    "df.columns = ['ARG', 'ARG-RAW', 'count']\n",
    "\n",
    "# Drop empty semantic phrases\n",
    "\n",
    "df = df[df['ARG'] != ''] \n",
    "\n",
    "# Rearrange the data\n",
    "\n",
    "df = df.groupby(['ARG']).apply(lambda x: x.sort_values([\"count\"], ascending = False))\n",
    "df = df.reset_index(drop= True)\n",
    "df = df.groupby(['ARG']).head(10)\n",
    "\n",
    "df['ARG-RAW'] = df['ARG-RAW'] + ' - ' + df['count'].astype(str)\n",
    "df['cluster_elements'] = df.groupby(['ARG'])['ARG-RAW'].transform(lambda x: ' | '.join(x))\n",
    "\n",
    "df = df.drop_duplicates(subset=['ARG'])\n",
    "\n",
    "df['cluster_elements'] = [', '.join(set(i.split(','))) for i in list(df['cluster_elements'])]\n",
    "\n",
    "print('Entities to inspect:', len(df))\n",
    "\n",
    "df = df[['ARG', 'cluster_elements']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de6015",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for l in df.values.tolist():\n",
    "    print('entity: \\n %s \\n' % [0])\n",
    "    print('most frequent phrases: \\n %s \\n' % l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6800a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-dimensional vs. high-dimensional narrative statements\n",
    "\n",
    "# Replace negated verbs by \"not-verb\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "final_statements['B-V_lowdim_with_neg'] = np.where(final_statements['B-ARGM-NEG_lowdim'] == True, \n",
    "                                          'not-' + final_statements['B-V_lowdim'], \n",
    "                                          final_statements['B-V_lowdim'])\n",
    "\n",
    "final_statements['B-V_highdim_with_neg'] = np.where(final_statements['B-ARGM-NEG_highdim'] == True, \n",
    "                                           'not-' + final_statements['B-V_lowdim'], \n",
    "                                           final_statements['B-V_highdim'])\n",
    "\n",
    "# Concatenate high-dimensional narratives (with text preprocessing but no clustering)\n",
    "\n",
    "final_statements['narrative_highdim'] = (final_statements['ARG0_highdim'] + ' ' + \n",
    "                                         final_statements['B-V_highdim_with_neg'] + ' ' +  \n",
    "                                         final_statements['ARG1_highdim'])\n",
    "\n",
    "# Concatenate low-dimensional narratives (with clustering)\n",
    "\n",
    "final_statements['narrative_lowdim'] = (final_statements['ARG0_lowdim'] + ' ' + \n",
    "                                        final_statements['B-V_highdim_with_neg'] + ' ' + \n",
    "                                        final_statements['ARG1_lowdim'])\n",
    "\n",
    "# Focus on narratives with a ARG0-VERB-ARG1 structure (i.e. \"complete narratives\")\n",
    "\n",
    "indexNames = final_statements[(final_statements['ARG0_lowdim'] == '')|\n",
    "                             (final_statements['ARG1_lowdim'] == '')|\n",
    "                             (final_statements['B-V_lowdim_with_neg'] == '')].index\n",
    "\n",
    "complete_narratives = final_statements.drop(indexNames)\n",
    "\n",
    "complete_narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fe607",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = complete_narratives.merge(survey_df, how='left', left_on='doc', right_on='ResponseId')\n",
    "test[test['Q7.5'].isin(['Always reject', 'reject'])]['narrative_lowdim'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ad32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top high-dimensional complete narrative statements\n",
    "\n",
    "complete_narratives['narrative_highdim'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1957c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top low-dimensional complete narrative statements\n",
    "\n",
    "complete_narratives['narrative_highdim'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ten random complete narratives with and without dimension reduction\n",
    "#\n",
    "# Specifying a small number of clusters leads to a large dimension reduction \n",
    "# and may decrease cluster coherence, whilst specifying a large number of clusters\n",
    "# may produce cluster redundancy. \n",
    "#\n",
    "# The choice of the number of clusters is corpus and application specific \n",
    "# (and was chosen at random in this notebook).\n",
    "\n",
    "sample = complete_narratives.sample(10, random_state = 123).to_dict('records')\n",
    "\n",
    "for d in sample:\n",
    "    print('Original raw response: \\n %s \\n' %split_sentences[1][d['sentence']])\n",
    "    print('High-dimensional narrative: \\n %s \\n' %d['narrative_highdim'])\n",
    "    print('Low-dimensional narrative: \\n %s \\n' %d['narrative_lowdim'])\n",
    "    print('--------------------------------------------------- \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(final_statements['ARG1_highdim']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ed864",
   "metadata": {},
   "source": [
    "## Step 6: Visualization // Plotting narrative graphs\n",
    "----------------------------\n",
    "\n",
    "A collection of narrative statements has an intuitive network structure, in which the edges are verbs and the nodes are entities.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf726653",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = complete_narratives[[\"ARG0_lowdim\", \"ARG1_lowdim\", \"B-V_lowdim\"]]\n",
    "temp.columns = [\"ARG0\", \"ARG1\", \"B-V\"]\n",
    "temp = temp[(temp[\"ARG0\"] != \"\") & (temp[\"ARG1\"] != \"\") & (temp[\"B-V\"] != \"\")]\n",
    "temp = temp.groupby([\"ARG0\", \"ARG1\", \"B-V\"]).size().reset_index(name=\"weight\")\n",
    "temp = temp.sort_values(by=\"weight\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot low-dimensional complete narrative statements in a directed multi-graph\n",
    "\n",
    "from relatio.graphs import build_graph, draw_graph\n",
    "\n",
    "temp = complete_narratives[[\"ARG0_lowdim\", \"ARG1_lowdim\", \"B-V_lowdim\"]]\n",
    "temp.columns = [\"ARG0\", \"ARG1\", \"B-V\"]\n",
    "temp = temp[(temp[\"ARG0\"] != \"\") & (temp[\"ARG1\"] != \"\") & (temp[\"B-V\"] != \"\")]\n",
    "temp = temp.groupby([\"ARG0\", \"ARG1\", \"B-V\"]).size().reset_index(name=\"weight\")\n",
    "temp = temp.sort_values(by=\"weight\", ascending=False).iloc[\n",
    "    0:100\n",
    "]  # pick top 100 most frequent narratives\n",
    "temp = temp.to_dict(orient=\"records\")\n",
    "\n",
    "for l in temp:\n",
    "    l[\"color\"] = None\n",
    "\n",
    "G = build_graph(\n",
    "    dict_edges=temp, dict_args={}, edge_size=None, node_size=10, prune_network=False #rue\n",
    ")\n",
    "\n",
    "draw_graph(G, notebook=True, output_filename=\"all_data_relatio_result_nltk_stops_no_1_letter_filter_k50_all_named_ents_prune_network_false_top100narr.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_statements['ARG0_lowdim'].value_counts())\n",
    "print(final_statements['ARG1_lowdim'].value_counts())\n",
    "\n",
    "help(get_narratives)\n",
    "#final_statements['ARG1_highdim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c732833",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a934da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot high-dimensional complete narrative statements in a directed multi-graph\n",
    "\n",
    "from relatio.graphs import build_graph, draw_graph\n",
    "\n",
    "temp = complete_narratives[[\"ARG0_highdim\", \"ARG1_highdim\", \"B-V_highdim\"]]\n",
    "temp.columns = [\"ARG0\", \"ARG1\", \"B-V\"]\n",
    "temp = temp[(temp[\"ARG0\"] != \"\") & (temp[\"ARG1\"] != \"\") & (temp[\"B-V\"] != \"\")]\n",
    "temp = temp.groupby([\"ARG0\", \"ARG1\", \"B-V\"]).size().reset_index(name=\"weight\")\n",
    "temp = temp.sort_values(by=\"weight\", ascending=False).iloc[\n",
    "    0:200\n",
    "]  # pick top 100 most frequent narratives\n",
    "temp = temp.to_dict(orient=\"records\")\n",
    "\n",
    "for l in temp:\n",
    "    l[\"color\"] = None\n",
    "\n",
    "G = build_graph(\n",
    "    dict_edges=temp, dict_args={}, edge_size=None, node_size=10, prune_network=False #True\n",
    ")\n",
    "\n",
    "draw_graph(G, notebook=True, output_filename=\"all_data_relatio_result_nltk_stops_no_1_letter_filter_k50_all_named_ents_high_dim_prune_network_false_top200narr.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4ad9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot low-dimensional complete narrative statements in a directed multi-graph\n",
    "\n",
    "from relatio.graphs import build_graph, draw_graph\n",
    "\n",
    "temp = final_statements[[\"ARG0_lowdim\", \"ARG1_lowdim\", \"B-V_lowdim\"]]\n",
    "temp.columns = [\"ARG0\", \"ARG1\", \"B-V\"]\n",
    "temp = temp[(temp[\"ARG0\"] != \"\") & (temp[\"ARG1\"] != \"\") & (temp[\"B-V\"] != \"\")]\n",
    "temp = temp.groupby([\"ARG0\", \"ARG1\", \"B-V\"]).size().reset_index(name=\"weight\")\n",
    "temp = temp.sort_values(by=\"weight\", ascending=False).iloc[\n",
    "    0:100\n",
    "]  # pick top 100 most frequent narratives\n",
    "temp = temp.to_dict(orient=\"records\")\n",
    "\n",
    "for l in temp:\n",
    "    l[\"color\"] = None\n",
    "\n",
    "G = build_graph(\n",
    "    dict_edges=temp, dict_args={}, edge_size=None, node_size=10, prune_network=False #True\n",
    ")\n",
    "\n",
    "draw_graph(G, notebook=True, output_filename=\"final_statements_initial_relatio_result_nltk_stops_no_1_letter_filter_k50_low_dim_prune_false.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a final comment, note that you can look up the specifics of any function with the help command.\n",
    "\n",
    "help(build_narrative_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b87b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
