{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e083aa1",
   "metadata": {},
   "source": [
    "## SPUR Densification Survey- Natural Langugage Processing Project\n",
    "notebook author: Emily Robitschek\n",
    "\n",
    "#### Background information on survey: \n",
    "This survey was conducted by the SPUR group at ETH: https://spur.ethz.ch/\n",
    "\n",
    "#### Hypothesis/Research question: \n",
    "Can we use the open text field responses from densification surveys to infer something about the motivations behind why individuals responded the way they did to the densification survey?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef49fc1",
   "metadata": {},
   "source": [
    "## Stage 3: Topic modelling of the pre-processed data with LDA\n",
    "To move beyond top individual words and better characterize themes themes or “topics” in the responses that don’t rely as much on a pre-stratification of the data along a particular response category, I carried out further preprocessing and topic modelling on the text data. Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds some natural groups of items (topics). Topic modeling techniques (specifically Latent Dirichlet Allocation or LDA) have been successfully applied in the context of NLP and text data to extract the main themes from the document collection without having to define them based on a priori expectations, although such methods have their limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import necessary packages: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import seaborn as sns\n",
    "from matplotlib import cm \n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# NLTK Stop words (extend if desired)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['proposal', 'proposals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "#from this post: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#18dominanttopicineachsentence\n",
    "\n",
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "        \n",
    "def get_perplexity_coherence(lda_model, corpus, data_lemmatized, id2word, coherence='c_v'): \n",
    "    \"\"\"Return perplexity and coherence for lda model.\"\"\"\n",
    "    # Compute Perplexity\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return perplexity, coherence_lda\n",
    "    \n",
    "\n",
    "def score_lda_model(min_topic_num, max_topic_num, lda_model, corpus, data_lemmatized, id2word):\n",
    "    \"\"\"Return perplexity and coherence for different number of topics for lda model.\"\"\"\n",
    "    perplexities = []\n",
    "    coherences = []\n",
    "    topic_nums_to_try = list(range(min_topic_num, max_topic_num))\n",
    "    for num in topic_nums_to_try: \n",
    "        print('Running LDA with %d topics' % (num))\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        perplexity, coherence = get_perplexity_coherence(lda_model, corpus, data_lemmatized, id2word, coherence='c_v')\n",
    "        perplexities.append(perplexity)\n",
    "        coherences.append(coherence)\n",
    "    lda_model_eval_df = pd.DataFrame({'number_of_topics': topic_nums_to_try, \n",
    "                                 'perplexity': perplexities, \n",
    "                                 'coherence': coherences})\n",
    "    return lda_model_eval_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fca5e0",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_col = 'Q12.6_corrected' #specify corrected text column\n",
    "survey_df = pd.read_csv('../../datasets/spur_survey_response_filtered_df1.txt', sep='\\t')\n",
    "survey_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18133323",
   "metadata": {},
   "source": [
    "### Preprocess data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify text data\n",
    "data = survey_df[subset_col]\n",
    "pprint(data[:1])\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=80) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=80)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f796ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072cb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary \n",
    "id2word = corpora.Dictionary(data_lemmatized)  \n",
    "# Create Corpus \n",
    "texts = data_lemmatized  \n",
    "# Term Document Frequency \n",
    "corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "# View \n",
    "print(corpus)\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09da50e",
   "metadata": {},
   "source": [
    "### Build model (LDA) and assess performance across K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb346ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c07003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_eval_df = score_lda_model(5, 14, lda_model, corpus, data_lemmatized, id2word)\n",
    "lda_model_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot perplexity across K\n",
    "sns.lineplot(data=lda_model_eval_df, x=\"number_of_topics\", y=\"perplexity\", \n",
    "             color='red', linewidth=2.5)\n",
    "title = (\"Perplexity scores for LDA models with different numbers of predicted topics\")\n",
    "plt.xticks(lda_model_eval_df[\"number_of_topics\"])\n",
    "plt.title(title)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plot coherence across K\n",
    "sns.lineplot(data=lda_model_eval_df, x=\"number_of_topics\", y=\"coherence\",\n",
    "            color='blue', linewidth=2.5)\n",
    "title = (\"Coherence scores for LDA models with different numbers of predicted topics\")\n",
    "plt.xticks(lda_model_eval_df[\"number_of_topics\"])\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=11, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52129461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 11 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd13042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "# Visualize the topics\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, sort_topics=False)\n",
    "print(vis_data.topic_order)\n",
    "vis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ca0e5",
   "metadata": {},
   "source": [
    "### Build model (LDAMallet) and assess performance across K\n",
    "https://stackoverflow.com/questions/62581874/gensim-ldamallet-vs-ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0415f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '../../resources/mallet-2.0.8/bin/mallet'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea84f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fedc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using this function that can take a long time to run from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ \n",
    "#because mine includes perplexity and that gave me some initial issues\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, \n",
    "                                                        texts=data_lemmatized, start=5, limit=14, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31238b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=14; start=5; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30969078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646000cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=12, id2word=id2word)\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = ldamallet \n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af801aa",
   "metadata": {},
   "source": [
    "### Relate the topics back to the individual responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa693c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,12):\n",
    "    print(\"Topic_\", i)\n",
    "    print(sent_topics_sorteddf_mallet['Keywords'][i])\n",
    "    print(sent_topics_sorteddf_mallet['Text'][i])\n",
    "    print( )\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,12):\n",
    "    print(\"Topic_\", i)\n",
    "    print(\"Top 10 Keywords:\", sent_topics_sorteddf_mallet['Keywords'][i])\n",
    "    print(\"EXAMPLE: \", \"'\", sent_topics_sorteddf_mallet['Text'][i], \"'\")\n",
    "    print( )\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics.head(12) #.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e9076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topics['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9cd58",
   "metadata": {},
   "source": [
    "### Merge topic data and survey data to be able to look at dominant topic prevalance compared to other survey responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7057a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df_w_lda = survey_df.merge(df_dominant_topic, left_index=True, right_index=True)\n",
    "survey_df_w_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df_w_lda[df_dominant_topic['Dominant_Topic']==3.0]['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce0226",
   "metadata": {},
   "source": [
    "### Climate subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aadf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby by climate\n",
    "climate = survey_df_w_lda.groupby('Q14.9')\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "climate.size().sort_values(ascending=False).plot.bar()\n",
    "plt.xticks(rotation=50)\n",
    "plt.xlabel(\"Climate concern\")\n",
    "plt.ylabel(\"Response count\")\n",
    "plt.show()\n",
    "\n",
    "# Summary statistic of all countries\n",
    "climate['Dominant_Topic'].describe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ecda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_mapper = {'Not concerned': -2, 'Rather not concerned': -1, 'Neither': 0, 'Quite concerned': 1, 'Very concerned': 2}\n",
    "survey_df_w_lda[\"level_climate_concern\"] = survey_df_w_lda[\"Q14.9\"].replace(scale_mapper)\n",
    "survey_df_w_lda\n",
    "\n",
    "summary_df = survey_df_w_lda[[\"level_climate_concern\", 'Dominant_Topic']] #)#['Dominant_Topic']#.count()\n",
    "print(len(summary_df))\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_prop = pd.crosstab(index=summary_df['level_climate_concern'],\n",
    "                             columns=summary_df['Dominant_Topic'],\n",
    "                             normalize=\"index\")\n",
    "climate_v_lda = cross_tab_prop\n",
    "climate_v_lda['level_climate_concern'] = climate_v_lda.index.copy()\n",
    "#cross_tab_prop = cross_tab_prop.reindex(index = ['Not concerned', 'Rather not concerned', 'Neither', 'Quite concerned', 'Very concerned'])\n",
    "cross_tab_prop #cross_tab_prop.sort_values('very-not concerned', ascending=False) #.corr()\n",
    "climate_v_lda.corr()\n",
    "df_corr = climate_v_lda.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f02e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make heatmap of plot above\n",
    "plt.figure(figsize=(12, 12))\n",
    "corr = df_corr.corr(method='spearman') #[['A']]\n",
    "ax = sns.clustermap(corr, #corr[[\"level_climate_concern\"]],\n",
    "                 robust=True, \n",
    "                 annot=True,  \n",
    "                 cmap=\"YlGnBu\")\n",
    "\n",
    "title = (\"Correlation heatmap of climate concern and dominant LDA topic\" )\n",
    "plt.title(title)\n",
    "file_name=('../../figures/' + title + ('_v1_spearman.png'))\n",
    "#plt.savefig((file_name), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2528c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = survey_df_w_lda[['Q14.9', 'Dominant_Topic']] #)#['Dominant_Topic']#.count()\n",
    "print(len(summary_df))\n",
    "summary_df.head()\n",
    "\n",
    "cross_tab_prop = pd.crosstab(index=summary_df['Dominant_Topic'],\n",
    "                             columns=summary_df['Q14.9'],\n",
    "                             normalize=\"index\")\n",
    "\n",
    "cross_tab_prop['very-not concerned'] =  cross_tab_prop['Very concerned'] - cross_tab_prop['Not concerned'] #.describe()\n",
    "cross_tab_prop.sort_values('very-not concerned', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a08b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make colormap: \n",
    "clrs = cm.get_cmap('RdYlBu_r', 22)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "cross_tab_prop.plot(kind='line', \n",
    "                    colormap=clrs, \n",
    "                    figsize=(10, 6))\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.0), ncol=2)\n",
    "#plt.xlabel(\"Release Year\")\n",
    "plt.ylabel(\"Proportion for each Q14.9 response cat\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4327992",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_prop = pd.crosstab(index=summary_df['Q14.9'],\n",
    "                             columns=summary_df['Dominant_Topic'],\n",
    "                             normalize=\"index\")\n",
    "\n",
    "cross_tab_prop = cross_tab_prop.reindex(index = ['Not concerned', 'Rather not concerned', 'Neither', 'Quite concerned', 'Very concerned'])\n",
    "\n",
    "cross_tab = pd.crosstab(index=summary_df['Q14.9'],\n",
    "                        columns=summary_df['Dominant_Topic'])\n",
    "\n",
    "cross_tab = cross_tab.reindex(index = ['Not concerned', 'Rather not concerned', 'Neither', 'Quite concerned', 'Very concerned'])\n",
    "cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "cross_tab_prop.plot(kind='line', \n",
    "                    colormap=clrs, \n",
    "                    figsize=(10, 6))\n",
    "plt.legend(bbox_to_anchor=(1.25, 1.00), ncol=2)\n",
    "plt.xlabel(\"climate concern\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9076ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = survey_df_w_lda[['Q14.9', 'Dominant_Topic']] #)#['Dominant_Topic']#.count()\n",
    "print(len(summary_df))\n",
    "summary_df.head()\n",
    "\n",
    "cross_tab_prop = pd.crosstab(index=summary_df['Q14.9'],\n",
    "                             columns=summary_df['Dominant_Topic'],\n",
    "                             normalize=\"index\")\n",
    "\n",
    "cross_tab_prop = cross_tab_prop.reindex(index = ['Not concerned', 'Rather not concerned', 'Neither', 'Quite concerned', 'Very concerned'])\n",
    "cross_tab_prop \n",
    "\n",
    "cross_tab= pd.crosstab(index=summary_df['Q14.9'],\n",
    "                             columns=summary_df['Dominant_Topic'])\n",
    "\n",
    "cross_tab = cross_tab.reindex(index = ['Not concerned', 'Rather not concerned', 'Neither', 'Quite concerned', 'Very concerned'])\n",
    "cross_tab\n",
    "\n",
    "cross_tab_prop.plot(kind='bar', \n",
    "                    stacked=True, \n",
    "                    colormap=clrs, \n",
    "                    figsize=(8, 8))\n",
    "\n",
    "\n",
    "for n, x in enumerate([*cross_tab.index.values]):\n",
    "    for (proportion, y_loc) in zip(cross_tab_prop.loc[x],\n",
    "                                   cross_tab_prop.loc[x].cumsum()):\n",
    "                \n",
    "        plt.text(x=n - 0.17,\n",
    "                 y=(y_loc - proportion) + (proportion / 5),\n",
    "                 s=f'{np.round(proportion * 100, 1)}%', \n",
    "                 color=\"black\",\n",
    "                 fontsize=10) #,\n",
    "                 #fontweight=\"bold\")\n",
    "\n",
    "plt.legend(loc=\"upper left\", ncol=2)\n",
    "plt.xlabel(\"Q14.9 How concerned are you about climate change?\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend(title='Dominant LDA Topic', bbox_to_anchor=(1.025, 1.00), ncol=2)\n",
    "fig_name = 'stacked_bar'\n",
    "file_name=('../../figures/' + fig_name + ('_ldamallet_12_topic_proportion_across_Q14.9.png'))\n",
    "#plt.savefig((file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b6295",
   "metadata": {},
   "source": [
    "### Acceptance/Rejection of densification projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c492c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = survey_df_w_lda[['Q7.5', 'Dominant_Topic']] #)#['Dominant_Topic']#.count()\n",
    "print(len(summary_df))\n",
    "summary_df.head()\n",
    "\n",
    "cross_tab_prop = pd.crosstab(index=summary_df['Q7.5'],\n",
    "                             columns=summary_df['Dominant_Topic'],\n",
    "                             normalize=\"index\")\n",
    "\n",
    "cross_tab_prop = cross_tab_prop.reindex(index = ['always reject', 'reject', 'mostly reject', 'undecided', 'mostly accept', 'accept', 'always accept'])\n",
    "cross_tab_prop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab= pd.crosstab(index=summary_df['Q7.5'],\n",
    "                             columns=summary_df['Dominant_Topic'])\n",
    "\n",
    "cross_tab = cross_tab.reindex(index = ['always reject', 'reject', 'mostly reject', 'undecided', 'mostly accept', 'accept', 'always accept'])\n",
    "cross_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4856d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "cross_tab_prop.plot(kind='line', \n",
    "                    colormap=clrs, \n",
    "                    figsize=(10, 6))\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.0), ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5bac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = survey_df_w_lda[['Q7.5', 'Dominant_Topic']] #)#['Dominant_Topic']#.count()\n",
    "print(len(summary_df))\n",
    "summary_df.head()\n",
    "\n",
    "cross_tab_prop = pd.crosstab(index=summary_df['Q7.5'],\n",
    "                             columns=summary_df['Dominant_Topic'],\n",
    "                             normalize=\"index\")\n",
    "\n",
    "cross_tab_prop = cross_tab_prop.reindex(index = ['always reject', 'reject', 'mostly reject', 'undecided', 'mostly accept', 'accept', 'always accept'])\n",
    "\n",
    "cross_tab= pd.crosstab(index=summary_df['Q7.5'],\n",
    "                             columns=summary_df['Dominant_Topic'])\n",
    "\n",
    "cross_tab = cross_tab.reindex(index = ['always reject', 'reject', 'mostly reject', 'undecided', 'mostly accept', 'accept', 'always accept'])\n",
    "\n",
    "\n",
    "clrs = cm.get_cmap('RdYlBu_r', 22)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "cross_tab_prop.plot(kind='bar', \n",
    "                    stacked=True, \n",
    "                    colormap=clrs, \n",
    "                    figsize=(10, 6))\n",
    "\n",
    "\n",
    "for n, x in enumerate([*cross_tab.index.values]):\n",
    "    for (proportion, y_loc) in zip(cross_tab_prop.loc[x],\n",
    "                                   cross_tab_prop.loc[x].cumsum()):\n",
    "                \n",
    "        plt.text(x=n - 0.17,\n",
    "                 y=(y_loc - proportion) + (proportion / 5),\n",
    "                 s=f'{np.round(proportion * 100, 1)}%', \n",
    "                 color=\"black\",\n",
    "                 fontsize=10) #,\n",
    "                 #fontweight=\"bold\")\n",
    "\n",
    "plt.legend(loc=\"upper left\", ncol=2)\n",
    "plt.xlabel(\"Q7.5 Do you personally tend to accept or reject projects that aim to increase building density?\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend(title='Dominant LDA Topic', bbox_to_anchor=(1.025, 1.00), ncol=2)\n",
    "fig_name = 'stacked_bar'\n",
    "file_name=('../../figures/' + fig_name + ('_ldamallet_12_topic_proportion_across_Q7.5.png'))\n",
    "#plt.savefig((file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0445d19",
   "metadata": {},
   "source": [
    "### Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = survey_df_w_lda[['frame', 'Dominant_Topic']] #)#['Dominant_Topic']#.count()\n",
    "print(len(summary_df))\n",
    "summary_df.head()\n",
    "\n",
    "cross_tab_prop = pd.crosstab(index=summary_df['frame'],\n",
    "                             columns=summary_df['Dominant_Topic'],\n",
    "                             normalize=\"index\")\n",
    "\n",
    "cross_tab_prop = cross_tab_prop.reindex(index = ['neighborhood', 'district', 'other district'])\n",
    "cross_tab_prop \n",
    "\n",
    "cross_tab= pd.crosstab(index=summary_df['frame'],\n",
    "                             columns=summary_df['Dominant_Topic'])\n",
    "\n",
    "cross_tab_prop = cross_tab_prop.reindex(index = ['neighborhood', 'district', 'other district'])\n",
    "cross_tab\n",
    "\n",
    "cross_tab_prop.plot(kind='bar', \n",
    "                    stacked=True, \n",
    "                    colormap=clrs, \n",
    "                    figsize=(4, 8))\n",
    "\n",
    "\n",
    "for n, x in enumerate([*cross_tab.index.values]):\n",
    "    for (proportion, y_loc) in zip(cross_tab_prop.loc[x],\n",
    "                                   cross_tab_prop.loc[x].cumsum()):\n",
    "                \n",
    "        plt.text(x=n - 0.17,\n",
    "                 y=(y_loc - proportion) + (proportion / 3),\n",
    "                 s=f'{np.round(proportion * 100, 1)}%', \n",
    "                 color=\"black\",\n",
    "                 fontsize=10) #,\n",
    "                 #fontweight=\"bold\")\n",
    "\n",
    "plt.legend(loc=\"upper left\", ncol=2)\n",
    "plt.xlabel(\"frame: Project location\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend(title='Dominant LDA Topic', bbox_to_anchor=(1.025, 1.00), ncol=2)\n",
    "fig_name = 'stacked_bar'\n",
    "file_name=('../../figures/' + fig_name + ('_ldamallet_12_topic_proportion_across_frame.png'))\n",
    "#plt.savefig((file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde86d6",
   "metadata": {},
   "source": [
    "### Political leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = survey_df_w_lda[['Q14.1_1', 'Dominant_Topic']] #)#['Dominant_Topic']#.count()\n",
    "print(len(summary_df))\n",
    "summary_df.head()\n",
    "\n",
    "cross_tab_prop = pd.crosstab(index=summary_df['Q14.1_1'],\n",
    "                             columns=summary_df['Dominant_Topic'],\n",
    "                             normalize=\"index\")\n",
    "\n",
    "\n",
    "cross_tab= pd.crosstab(index=summary_df['Q14.1_1'],\n",
    "                             columns=summary_df['Dominant_Topic'])\n",
    "\n",
    "\n",
    "cross_tab_prop.plot(kind='bar', \n",
    "                    stacked=True, \n",
    "                    colormap=clrs, \n",
    "                    figsize=(12, 8))\n",
    "\n",
    "\n",
    "for n, x in enumerate([*cross_tab.index.values]):\n",
    "    for (proportion, y_loc) in zip(cross_tab_prop.loc[x],\n",
    "                                   cross_tab_prop.loc[x].cumsum()):\n",
    "                \n",
    "        plt.text(x=n - 0.17,\n",
    "                 y=(y_loc - proportion) + (proportion / 3),\n",
    "                 s=f'{np.round(proportion * 100, 1)}%', \n",
    "                 color=\"black\",\n",
    "                 fontsize=10) #,\n",
    "                 #fontweight=\"bold\")\n",
    "\n",
    "plt.legend(loc=\"upper left\", ncol=2)\n",
    "plt.xlabel(\"political leaning 0- most left and 7- most right\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend(title='Dominant LDA Topic', bbox_to_anchor=(1.025, 1.00), ncol=2)\n",
    "fig_name = 'stacked_bar'\n",
    "file_name=('../../figures/' + fig_name + ('_ldamallet_12_topic_proportion_across_Q14.1_1.png'))\n",
    "#plt.savefig((file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcab6d4",
   "metadata": {},
   "source": [
    "### City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = survey_df_w_lda[['city', 'Dominant_Topic']] #)#['Dominant_Topic']#.count()\n",
    "print(len(summary_df))\n",
    "summary_df.head()\n",
    "\n",
    "cross_tab_prop = pd.crosstab(index=summary_df['city'],\n",
    "                             columns=summary_df['Dominant_Topic'],\n",
    "                             normalize=\"index\")\n",
    "\n",
    "\n",
    "cross_tab= pd.crosstab(index=summary_df['city'],\n",
    "                             columns=summary_df['Dominant_Topic'])\n",
    "\n",
    "\n",
    "cross_tab_prop.plot(kind='bar', \n",
    "                    stacked=True, \n",
    "                    colormap=clrs, \n",
    "                    figsize=(6, 8))\n",
    "\n",
    "\n",
    "for n, x in enumerate([*cross_tab.index.values]):\n",
    "    for (proportion, y_loc) in zip(cross_tab_prop.loc[x],\n",
    "                                   cross_tab_prop.loc[x].cumsum()):\n",
    "                \n",
    "        plt.text(x=n - 0.17,\n",
    "                 y=(y_loc - proportion) + (proportion / 3),\n",
    "                 s=f'{np.round(proportion * 100, 1)}%', \n",
    "                 color=\"black\",\n",
    "                 fontsize=10) #,\n",
    "                 #fontweight=\"bold\")\n",
    "\n",
    "plt.legend(loc=\"upper left\", ncol=2)\n",
    "plt.xlabel(\"City\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend(title='Dominant LDA Topic', bbox_to_anchor=(1.025, 1.00), ncol=2)\n",
    "fig_name = 'stacked_bar'\n",
    "#file_name=('../../figures/' + fig_name + ('_ldamallet_12_topic_proportion_across_city2.png'))\n",
    "plt.savefig((file_name), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ff9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e71df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
